---
title: 머신러닝 공부 - 랜덤 포레스트(Random Forest) 쉽게 이해하기
date: 2020-01-16
categories: [WORK, Data]
tags: [데이터분석, MachineLearning, Python]
---

의사결정 나무는 매우 훌륭한 모델이지만, 학습 데이터에 오버피팅 하는 경향이 있다. 가지치기(pruning) 같은 방법을 통해 그런 부작용을 최소하하는 전략이 있긴 하나 역시나 좀 부족하다.

그래서 본 포스팅에서는 의사결정 트리의 오버피팅 한계를 극복하기 위한 전략으로 랜덤 포레스트(Random Forest)를 소개하고자 한다.

## 랜덤 포레스트란

(“무작위 숲”이라는 이름처럼) 랜덤 포레스트는 훈련을 통해 구성해놓은 다수의 나무들로부터 분류 결과를 취합해서 결론을 얻는, 일종의 인기 투표 같은 거다.

아래 그림처럼.

![random forest](https://upload.wikimedia.org/wikipedia/commons/4/4e/Random_forest_explain.png)

물론 몇몇의 나무들이 오버피팅을 보일 순 있지만 다수의 나무를 기반으로 예측하기 때문에 그 영향력이 줄어들게 된어 좋은 일반화 성능을 보인다.

이렇게 좋은 성능을 얻기 위해 다수의 학습 알고리즘을 사용하는 걸 앙상블(ensemble) 학습법이라고 부른다.

일단 랜덤 포레스트에서 각 나무들을 어떻게 생성하는지 알아야 한다. 결론부터 얘기하면… 배깅(bagging)이라는 프로세스를 통해 나무를 만든다.

## 배깅(Bagging)

학습 데이터 세트에 총 1000개의 행이 있다고 해보자. 그러면 임의로 100개씩 행을 선택해서 의사결정 트리를 만드는 게 배깅(bagging)이다. 물론 이런 식으로 트리를 만들면 모두 다르겠지만 그래도 어쨌거나 학습 데이터의 일부를 기반으로 생성했다는 게 중요하다.

그리고 이 때 중복을 허용해야 한다는 걸 기억하자.

1000개의 행이 있는 가방(bag)에서 임의로 100개 뽑아 첫 번째 트리를 만들고 그 100개의 행은 가방에 도로 집어 넣는다. 그리고 다시 1000개의 행에서 또 임의로 100개를 뽑아 두 번째 트리를 만든 후 다시 가방에 집어 넣고 뭐 이런 식.

여기에 트리를 만들 때는 사용될 속성(feature)들을 제한함으로써 각 나무들에 다양성을 주는 게 중요하다.

원래는 트리를 만들 때 모든 속성들을 살펴보고 정보 획득량이 가장 많은 속성을 선택해서 그걸 기준으로 데이터를 분할했다. 그러나 이제는 각 분할에서 전체 속성들 중 일부만 고려하여 트리를 작성하도록 하는 전략이다.

예를 들면 총 25개의 속성이 있는데, 그 중 5개의 속성만 뽑아서 살펴본 후 그 중 정보 획득량이 가장 높은 걸 기준으로 데이터를 분할하는 거다. 그 다음 단계에서도 다시 임의로 5개만 선택해서 살펴보는 식.

그렇다면 몇 개씩 속성을 뽑는 게 좋을까. 위 예처럼 총 속성이 25개면 5개, 즉 전체 속성 개수의 제곱근만큼 선택하는 게 가장 좋다고, 경험적으로 그렇게 나타난다고 한다. (일종의 a rule of thumb이다.)

이제 서로 다른 트리를 만들 수 있게 되었으니 이것들을 모아 ‘숲’을 이루도록 하면 되는 거다.

## scikit-learn 사용법

랜덤 포레스트는 파이썬 라이브러리 scikit-learn을 사용하면 쉽게 구현할 수 있다.

`sklearn.ensemble` 모듈에서 `RandomForestClassifier`를 불러오면 된다. 단, 숲을 만들 때 나무의 개수를 `n_estimators`라는 파라미터로 지정해주어야 한다.

```python
from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier(n_estimators = 100)
```

`RandomForestClassifier`는 의사결정 나무 `DecisionTreeClassifier`와 거의 똑같다. 당연히 `.fit()`, `.predict()`, `.score()` 같은 메서드를 사용할 수 있다.

## 요약

- 랜덤 포레스트는 앙상블 머신러닝 모델이다. 다수의 의사결정 트리를 만들고, 그 나무들의 분류를 집계해서 최종적으로 분류한다.
- 오버피팅을 피하기 위해 임의(random)의 숲을 구성하는 거다. 다수의 나무들로부터 분류를 집계하기 때문에 오버피팅이 나타나는 나무의 영향력을 줄일 수 있다.
- 모든 의사결정 트리는 학습 데이터 세트에서 임의로 하위 데이터 세트를 추출하여 생성된다. 중복을 허용하기 때문에 단일 데이터가 여러번 선택될 수도 있다. 이 과정을 배깅(bagging)이라고 한다.
- 나무를 만들 때는 모든 속성(feature)들에서 임의로 일부를 선택하고 그 중 정보 획득량이 가장 높은 것을 기준으로 데이터를 분할한다. 만약 데이터 세트에 n개의 속성이 있는 경우 n제곱근 개수만큼 무작위로 선택하는 것이 일반적이다.